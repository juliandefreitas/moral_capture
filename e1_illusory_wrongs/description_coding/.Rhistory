if (!require(ltm)) {install.packages("ltm"); require(ltm)}
if (!require(heplots)) {install.packages("heplots"); require(heplots)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(jsonlite)) {install.packages("jsonlite"); require(jsonlite)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lme4)) {install.packages("lme4"); require(lme4)}
if (!require(reshape2)) {install.packages("reshape2"); require(reshape2)}
if (!require(RColorBrewer)) {install.packages("RColorBrewer"); require(RColorBrewer)}
if (!require(Hmisc)) {install.packages("Hmisc"); require(Hmisc)}
# two experiment directories
protoDir <- 'e1_speededVunspeeded'
badPDir <- 'e3_speededVunspeeded_badPatients'
## set directory to data folder
julianDir <- "/Users/Julian/Dropbox (Personal)/event_morality/experiments"
alonDirPart <- "Dropbox/GRAD_SCHOOL/DATED_DOCS/2017_SPRING/PROJECTS/event_morality/experiments"
if (dir.exists(julianDir)) {
fullDir <- julianDir
} else {
if (.Platform$OS.type=="unix") {
dirPrefix <- "/Users/alonhafri"
} else {
dirPrefix <- "C:/Users/Alon"
}
fullDir <- file.path(dirPrefix, alonDirPart)
}
###########################################
###########################################
## load data
data.proto <- read.csv(file.path(fullDir,protoDir,'moralAtoms_speeded_subj_dprimes.csv'),
header=T)
data.badP <- read.csv(file.path(fullDir,badPDir,'moralAtoms_badPatient_subj_dprimes.csv'),
header=T)
data.proto$experiment <- 'proto'
data.badP$experiment <- 'bad-patient'
# merge data
data.all.subj <- rbind(data.proto, data.badP)
data.all.subj$experiment <- factor(data.all.subj$experiment)
data.all.subj$experiment <- factor(data.all.subj$experiment, levels(data.all.subj$experiment)[c(2,1)])
data.all.subj$condName <- factor(data.all.subj$condName, levels(data.all.subj$condName)[c(1,4,2,3)])
###########################################
###########################################
## stats
anova.subj <- aov(dprime ~ condName * experiment, data = subset(data.all.subj))
summary(anova.subj)
t.test(dprime ~ experiment, subset(data.all.subj, condName=='color'), paired=F)
t.test(dprime ~ experiment, subset(data.all.subj, condName=='role'), paired=F)
t.test(dprime ~ experiment, subset(data.all.subj, condName=='harm'), paired=F)
t.test(dprime ~ experiment, subset(data.all.subj, condName=='moral'), paired=F)
anova.subj <- aov(dprime ~ condName * experiment, data = subset(data.all.subj, condName=='role' | condName=='harm'))
summary(anova.subj)
anova.subj <- aov(dprime ~ condName * experiment, data = subset(data.all.subj, condName=='harm' | condName=='moral'))
summary(anova.subj)
anova.subj <- aov(dprime ~ condName * experiment, data = subset(data.all.subj, condName=='role' | condName=='moral'))
summary(anova.subj)
###########################################
###########################################
## model data frame
# model data points (with approx. values of 0.98)
# proto: 3.57, 1.93, 0.65, 0.65 [not a typo]
# bad: 4.10, 0.81, 0.42, 0.01
protoVals <- c(3.57, 1.93, 0.65, 0.65)
badVals <- c(4.10, 0.81, 0.42, 0.01)
dprime <- c(protoVals, badVals)
condName <- rep(c('color','role','harm','moral'),2)
experiment <- c(rep('proto',4), rep('bad-patient',4))
modelVals <- data.frame(experiment, condName, dprime)
modelVals$experiment <- factor(modelVals$experiment)
modelVals$experiment <- factor(modelVals$experiment, levels(modelVals$experiment)[c(2,1)])
modelVals$condName <- factor(modelVals$condName)
###########################################
###########################################
## plot
pl <- ggplot(subset(data.all.subj,
condName!='moral.patient' & speedCond=='speeded'),
aes(x=condName, y=dprime, fill=experiment)) +
stat_summary(fun.y=mean,position=position_dodge(width=0.95),geom="bar", color='black') +
#geom_jitter(aes(fill=experiment),color='black',pch=21,size=2, alpha=0.5,show.legend=FALSE,
#           position=position_dodge(width=0.95),
#            position_jitter(width = .05))+
geom_point(aes(fill=experiment), color='black',pch=21,size=2, alpha=0.5, show.legend=FALSE,
position=position_jitterdodge(dodge.width=0.95)) +
stat_summary(aes(x=condName, y=dprime), geom="errorbar", position=position_dodge(width=0.95), fun.data="mean_cl_normal",
size=1, width=0) +
geom_hline(yintercept=0,colour = "black",size=1) +
scale_x_discrete(name=element_blank(),
labels=c('color' = 'Color\n(control)',
'role' = 'Role',
'harm' = 'Harm',
'moral' = 'Moral\nWrongness\n(Agent)')) +
#scale_y_continuous(name='D-prime',breaks=seq(-3,5,1)) +
scale_y_continuous(name='D-prime',limits = c(-1,4.5), breaks=seq(-1,5,1)) +
theme(panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line.y = element_line(colour = "black",size=1),
axis.title.y= element_text(color='black',size=30),
axis.title.x= element_text(color='black',size=30),
axis.text.y= element_text(color='black',size=24),
axis.text.x= element_text(color='black',size=30),
axis.ticks.x = element_blank()) +
theme(legend.position="top",legend.text=element_text(size=24),legend.title=element_text(size=30)) +
geom_point(data = modelVals, aes(condName, dprime), shape = 22, color='black', stroke = 2, size = 5, alpha=0.9,
position=position_dodge(width=0.99)) +
guides(size=FALSE)
pl
harm_predictions <- c(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0)
harm_predictions <- c( 0.5 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.5 ,
0.  ,  0.5 ,  0.  ,  0.  ,  0.  ,  0.5 ,  0.5 ,  0.5 ,  0.25,
1.  ,  0.  ,  0.  ,  0.5 ,  1.  ,  1.)
harm_predictions
harm_truths <- c(0.46,  0.48,  0.57,  0.12,  0.3 ,  0.44,  0.44,  0.41,  0.14,
0.21,  0.86,  0.25,  0.14,  0.13,  0.5 ,  0.86,  0.82,  0.95,
0.92,  0.87,  0.88,  0.6 ,  0.49,  0.86)
harm_truths
plot(harm_truths, harm_predictions)
abline(harm_predictions, harm_truths)
plot(harm_predictions, harm_truths)
harm_truths = model
harm_truths
model = harm_truths
humans = harm_truths
model = harm_predictions
model
plot(model, humans)
abline(lm(humans ~ model))
Model = harm_predictions
Humans = harm_truths
plot(Model, Humans)
abline(lm(humans ~ model))
cor.test(model, humans)
model = c( 0.5 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.25,
0.  ,  0.75,  0.  ,  0.  ,  0.  ,  0.25,  0.5 ,  0.5 ,  0.  ,
1.  ,  0.  ,  0.  ,  0.25,  1.  ,  1.)
humans = c(0.47,  0.6 ,  0.59,  0.1 ,  0.49,  0.44,  0.38,  0.46,  0.21,
0.3 ,  0.85,  0.31,  0.26,  0.1 ,  0.54,  0.94,  0.69,  0.91,
0.85,  0.78,  0.73,  0.53,  0.48,  0.68)
Model = model
Humans = human
Humans = humans
plot(Model, Humans)
abline(lm(humans ~ model))
cor.test(model, humans)
pred = c(0.5 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.5 ,
0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.5 ,  0.5 ,  0.5 ,  0.  ,
1.  ,  0.  ,  0.  ,  0.25,  1.  ,  1.  )
truths = c(0.47,  0.6 ,  0.59,  0.1 ,  0.49,  0.44,  0.38,  0.46,  0.21,
0.3 ,  0.85,  0.31,  0.26,  0.1 ,  0.54,  0.94,  0.69,  0.91,
0.85,  0.78,  0.73,  0.53,  0.48,  0.68)
cor.test(pred, truths)
diff_humans &lt;- c(0.4, 0.7, 0.1, 0.35) diff_model &lt;- c(-0.54, 1.47, 0.11, -0.49)
diff_humans <- c(0.4, 0.7, 0.1, 0.35)
diff_model <- c(-0.54, 1.47, 0.11, -0.49)
cor.test(diff_humans, diff_model)
diff_humans <- c(0.4, 0.7, 0.1, 0.35)
diff_model <- c(-0.54, 1.47, 0.11, 0.51)
cor.test(diff_humans, diff_model)
diff_model
diff_model[0]
diff_model[1]
diff_model[4] - 0.35
corr.test(diff_humans, diff_model)
cor.test(diff_humans, diff_model)
diff_humans <- c(0.4, 0.7, 0.1, 0.35)
diff_model <- c(0.54, 1.47, 0.11, 0.51)
cor.test(diff_humans, diff_model)
cor.test(diff_humans[2:], diff_model[2:])
diff_humans[2:]
diff_humans[2,]
diff_humans[2:,]
diff_humans
diff_humans[2,]
diff_humans[,2:]
diff_humans(,2:)
diff_humans
diff_humans_short <- c(0.70 0.10 0.35)
diff_humans_short <- c(0.70, 0.10, 0.35)
diff_model
diff_model_short <- c(1.47, 0.11, 0.51)
cor.test(diff_humans_short, diff_model_short)
humans <- c(3.2, 2.8, 2.5, 1.8, 1.3, 1.2, 1.25, 0.9)
model <- c(3.23, 3.77, 2.31, 0.84, 1.25, 1.14, 1.20, 0.69)
cor.test(humans, model)
diff_humans <- c(0.4, 0.7, 0.1, 0.35)
diff_model <- c(-0.54, 1.47, 0.11, 0.51)
cor.test(diff_humans, diff_model)
R.version()
R.Version()
plot_grid(p1, p2, labels = "AUTO")
if (!require(cowplot)) {install.packages("cowplot"); require(cowplot)}
## clear workspace
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
##================ import data: launch, no context, and launch context====================================================================================
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
#agreement between raters
pt_codes <- data$Looknoo_code
jdf_codes <- data$julian_code
cohen.kappa(x=cbind(pt_codes,jdf_codes))
labels(data)
#t-test: causal vs. non-causal sentence on average blame
data <- subset(data, data$agreement == 1)
len(causal[causal == 1])/len(causal)
causal <- data$causal
len(causal[causal == 1])/len(causal)
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_codes <- data$Looknoo_code
jdf_codes <- data$julian_code
causal <- data$causal
blame <- data$blame_scaled
#agreement between raters
cohen.kappa(x=cbind(pt_codes,jdf_codes))
#proportion that were causal
data <- subset(data, data$agreement == 1)
length(causal[causal == 1])/length(causal)
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
##================ import data: launch, no context, and launch context====================================================================================
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_codes <- data$Looknoo_code
jdf_codes <- data$julian_code
causal <- data$causal
blame <- data$blame_scaled
#agreement between raters
cohen.kappa(x=cbind(pt_codes,jdf_codes))
#proportion that were causal
data <- subset(data, data$agreement == 1)
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tapply(blame, causal, mean)
tapply(blame, causal, sd)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tapply(blame, causal, length)
tes(-21.468,132,265) #0.3
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
tes(-21.468,132,265) #0.3
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
tapply(blame, causal, sd)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tes(-21.468,132,265) #
## clear workspace
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
data <- read.csv('descriptions_e1_COMBINED_causal.csv')
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_causal_codes <- data$pt_causal_code
jdf_causal_codess <- data$jdf_causal_code
pt_causal_codes <- data$pt_causal_code
jdf_causal_codess <- data$jdf_causal_code
causal <- data$CAUSAL
blame <- data$blame_scaled
pt_realistic_codes <- data$pt_realistic_code
jdf_realistic_codes <- data$jdf_realistic_code
realistic <- data$REALISTIC
pt_causal_codes <- data$pt_causal_code
jdf_causal_codess <- data$jdf_causal_code
causal <- data$CAUSAL
blame <- data$blame_scaled
#agreement between raters
cohen.kappa(x=cbind(pt_causalL_codes,jdf_causal_codes))
#agreement between raters
cohen.kappa(x=cbind(pt_causal_codes,jdf_causal_codes))
pt_causal_codes <- data$pt_causal_code
jdf_causal_codes <- data$jdf_causal_code
#agreement between raters
cohen.kappa(x=cbind(pt_causal_codes,jdf_causal_codes))
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
#agreement between raters
cohen.kappa(x=cbind(pt_causal_codes,jdf_causal_codes)) #0.87
#proportion that were causal
data <- subset(data, data$agreement == 1)
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
pt_realistic_codes = pt_realistic_codes + 1
jdf_realistic_codes = jdf_realistic_codes + 1
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
pt_realistic_codes
pt_realistic_codes
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
## clear workspace
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
##================ import data: launch, no context, and launch context====================================================================================
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_causal_codes <- data$pt_causal_code
jdf_causal_codes <- data$jdf_causal_code
causal <- data$CAUSAL
pt_realistic_codes <- data$pt_realistic_code
jdf_realistic_codes <- data$jdf_realistic_code
realistic <- data$REALISTIC
blame <- data$blame_scaled
##=================== causal codes ==============================
#agreement between raters
cohen.kappa(x=cbind(pt_causal_codes,jdf_causal_codes)) #0.87
#proportion that were causal
data <- subset(data, data$agreement == 1)
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
tapply(blame, causal, sd)
tapply(blame, causal, length)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tes(-21.468,132,265) #2.29
##=================== realistic codes ===========================
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_causal_codes <- data$pt_causal_code
jdf_causal_codes <- data$jdf_causal_code
causal <- data$CAUSAL
pt_realistic_codes <- data$pt_realistic_code
jdf_realistic_codes <- data$jdf_realistic_code
realistic <- data$REALISTIC
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
pt_realistic_codes
jdf_realistic_codes
cohen.kappay(pt_realistic_codes, jdf_realistic_codes)
#agreement between raters
cohen.kappa(x=cbind(pt_realistic_codes,jdf_realistic_codes)) #0.87
Kappa.test(pt_realistic_codes, jdf_realistic_codes)
if (!require(fmsb)) {install.packages("fmsb"); require(fmsb)}
Kappa.test(pt_realistic_codes, jdf_realistic_codes)
Kappa.test(pt_causal_codes, jdg_causal_codes)
Kappa.test(pt_causal_codes, jdf_causal_codes)
labels(data)
causal_agreement <- data$causal_agreement
realistic_agreement <- data$realistic_agreement
#proportion that were causal
data <- subset(data, causal_agreement == 1)
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
tapply(blame, causal, sd)
tapply(blame, causal, length)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tes(-21.468,132,265) #2.29
#proportion that were unrealistic
data <- subset(data, realistic_agreement == 1)
#agreement between raters
Kappa.test(pt_realistic_codes, jdf_realistic_codes)
## clear workspace
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
if (!require(fmsb)) {install.packages("fmsb"); require(fmsb)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
##================ import data: launch, no context, and launch context====================================================================================
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_causal_codes <- data$pt_causal_code
jdf_causal_codes <- data$jdf_causal_code
causal_agreement <- data$causal_agreement
causal <- data$CAUSAL
pt_realistic_codes <- data$pt_realistic_code
jdf_realistic_codes <- data$jdf_realistic_code
realistic_agreement <- data$realistic_agreement
realistic <- data$REALISTIC
blame <- data$blame_scaled
##=================== causal codes ==============================
#agreement between raters
Kappa.test(pt_causal_codes, jdf_causal_codes)
#proportion that were causal
data <- subset(data, causal_agreement == 1)
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
tapply(blame, causal, sd)
tapply(blame, causal, length)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tes(-21.468,132,265) #2.29
##=================== realistic codes ===========================
#agreement between raters
Kappa.test(pt_realistic_codes, jdf_realistic_codes)
#proportion that were unrealistic
data <- subset(data, realistic_agreement == 1)
#agreement between raters
Kappa.test(pt_causal_codes, jdf_causal_codes)
#proportion that were unrealistic
data <- subset(data, realistic_agreement == 1)
#proportion that were causal
#data <- subset(data, causal_agreement == 1)
length(causal[causal == 1 & causal_agreement == 1])/length(causal[causal_agreement == 1])
## clear workspace
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
if (!require(fmsb)) {install.packages("fmsb"); require(fmsb)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
##================ import data: launch, no context, and launch context====================================================================================
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_causal_codes <- data$pt_causal_code
jdf_causal_codes <- data$jdf_causal_code
causal_agreement <- data$causal_agreement
causal <- data$CAUSAL
pt_realistic_codes <- data$pt_realistic_code
jdf_realistic_codes <- data$jdf_realistic_code
realistic_agreement <- data$realistic_agreement
realistic <- data$REALISTIC
blame <- data$blame_scaled
##=================== causal codes ==============================
#agreement between raters
Kappa.test(pt_causal_codes, jdf_causal_codes)
#proportion that were causal
#data <- subset(data, causal_agreement == 1)
length(causal[causal == 1 & causal_agreement == 1])/length(causal[causal_agreement == 1])
#proportion that were causal
#data <- subset(data, causal_agreement == 1)
length(causal[causal == 1 & causal_agreement == 1])/length(causal)
causal_agreement
len(causal)
length(causal)
length(causal[causal_agreement == 1])
#proportion that were causal
#data <- subset(data, causal_agreement == 1)
causal <- data$CAUSAL[causal_agreement == 1]
length(causal[causal == 1])/length(causal)
#agreement between raters
Kappa.test(pt_causal_codes, jdf_causal_codes)
#proportion that were causal
causal <- data$CAUSAL[causal_agreement == 1]
blame <- data$blame_scaled[causal_agreement == 1]
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
tapply(blame, causal, sd)
tapply(blame, causal, length)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tes(-21.468,132,265) #2.29
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
#proportion that were unrealistic
realistic <- data$REALISTIC[realistic_agreement == 1]
blame <- data$blame_scaled[realistic_agreement == 1]
length(realistic[realistic == 1])/length(realistic)
length(causal[causal == 1])/length(causal)
length(causal)
#proportion of sentences rated causal
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
## clear workspace
rm(list = ls())
## necessary libraries
if (!require(psych)) {install.packages("psych"); require(psych)}
if (!require(fmsb)) {install.packages("fmsb"); require(fmsb)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
##================ import data: launch, no context, and launch context====================================================================================
dir <- setwd("/Users/Julian/Dropbox (alvarezlab)/Research-DeFreitas/Projects/moralCapture/FinalExperiments_newExclusions/e1_illusory_moral_wrongs/e1_descriptionCoding")
data <- read.csv('descriptions_e1_COMBINED.csv')
labels(data)
pt_causal_codes <- data$pt_causal_code
jdf_causal_codes <- data$jdf_causal_code
causal_agreement <- data$causal_agreement
pt_realistic_codes <- data$pt_realistic_code
jdf_realistic_codes <- data$jdf_realistic_code
realistic_agreement <- data$realistic_agreement
realistic <- data$REALISTIC
##=================== causal codes ==============================
#agreement between raters
Kappa.test(pt_causal_codes, jdf_causal_codes)
#number of sentences where raters agreed
causal <- data$CAUSAL[causal_agreement == 1]
blame <- data$blame_scaled[causal_agreement == 1]
length(causal) #375
#proportion of sentences rated causal
length(causal[causal == 1])/length(causal)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mea
_)
#t-test: causal vs. non-causal sentence on average blame
tapply(blame, causal, mean)
tapply(blame, causal, sd)
tapply(blame, causal, length)
t.test(blame ~ causal, var.equal=TRUE, paired=FALSE)
tes(-25.173,111,264) #2.29
